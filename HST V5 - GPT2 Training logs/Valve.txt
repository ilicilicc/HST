import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple, Optional, List
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# ==================== CONFIGURATION (Final) ====================
# GPT-2 Small Config
D_MODEL = 768
N_HEADS = 12
N_LAYERS = 12 # MATCHING GPT-2 SMALL
MAX_SEQ_LEN = 1024 # Maximum tokens the standard Attention can see (context size for self-attention)
VOCAB_SIZE = 50257 

# HST Config
CHUNK_SIZE = 128
HORIZON = 8
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Training Config
TRAIN_STEPS = 10 # CRITICAL: Minimal Adaptation Step on prompt (Zero-Data Adaptation)
BATCH_SIZE = 4
LR = 1e-4

# Knowledge Distillation Config
KD_TEMPERATURE = 2.0
KD_ALPHA = 0.5 # Weight for KD Loss vs. Cross-Entropy Loss

# Generation Config
MAX_GEN_TOKENS = 2000
OUTPUT_FILENAME = "hst_v5_chunk_story_zero_data.txt"
TEMPERATURE = 0.8
TOP_P = 0.9 # CRITICAL for non-repetitive, high-quality long generation
PROMPT_TEXT = "The AI awoke and began to rewrite its own code," 

print(f"Running on: {DEVICE}")

# ==================== 1. LATTICE ANALYZER ====================
class RecursiveDescentLatticeAnalyzer(nn.Module):
    """Placeholder for the complex lattice structure logic (simplified for demonstration)"""
    def __init__(self, max_seq_len=8192):
        super().__init__()
        spine_list = self._generate_spine_list(max_seq_len)
        self.register_buffer('spine', torch.tensor(spine_list, dtype=torch.long))
        self.layer_weights = nn.Parameter(torch.ones(10))

    def _generate_spine_list(self, max_len):
        spine = [0, 2, 4]
        while True:
            # Fibonacci-like growth pattern for complexity
            next_val = 2 * spine[-1] + 2 * spine[-2] + 2 * spine[-3]
            if next_val >= max_len:
                break
            spine.append(next_val)
        return spine

    def forward(self, x):
        return x

# ==================== 2. CHUNK ENCODER/DECODER ====================
class ChunkEncoder(nn.Module):
    def __init__(self, d_model, chunk_size=128, n_heads=8, n_layers=2):
        super().__init__()
        self.chunk_size = chunk_size
        self.d_model = d_model
        # Local encoder (e.g., a small BERT-like model for local context)
        encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, d_model * 4, batch_first=True, dropout=0.1)
        self.local_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # Pooling mechanism to summarize the chunk
        self.pooling_query = nn.Parameter(torch.randn(1, 1, d_model))
        self.pooling_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)

    def forward(self, token_embeddings):
        B, total_tokens, D = token_embeddings.shape
        # Pad input to be a multiple of chunk_size
        if total_tokens % self.chunk_size != 0:
            pad_len = self.chunk_size - (total_tokens % self.chunk_size)
            token_embeddings = F.pad(token_embeddings, (0, 0, 0, pad_len))
        
        num_chunks = token_embeddings.shape[1] // self.chunk_size
        chunks = token_embeddings.view(B * num_chunks, self.chunk_size, D)
        
        # Local encoding
        encoded_tokens = self.local_encoder(chunks)
        
        # Attention pooling to get one summary vector per chunk
        query = self.pooling_query.expand(B * num_chunks, -1, -1)
        pooled, _ = self.pooling_attn(query, encoded_tokens, encoded_tokens)
        
        return pooled.view(B, num_chunks, D)

class ChunkDecoder(nn.Module):
    """
    Decodes the chunk summary vector back into the token space (simplified).
    Used primarily during training/adaptation for structural completeness.
    """
    def __init__(self, d_model, vocab_size, chunk_size=128, n_heads=8, n_layers=2):
        super().__init__()
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, chunk_embeddings, target_token_embeddings):
        # Placeholder: simply project the chunk embeddings for structural integrity
        B, num_chunks, D = chunk_embeddings.shape
        seq_len = num_chunks * 1 # Simple projection, not full decoding
        
        projected = self.lm_head(chunk_embeddings)
        
        # Resize/reshape to match expected output structure if needed, or return raw projection
        return projected

# ==================== 3. TRANSFORMER LAYERS (GPT-2 compatible) ====================
class SelfAttentionWithCache(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        # Separate QKV projections for explicit weight mapping from GPT-2's c_attn
        self.q_proj = nn.Linear(d_model, d_model, bias=False) 
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False) # GPT-2 c_proj
        
    def forward(self, x, layer_past: Optional[Tuple[torch.Tensor]] = None):
        B, S, D = x.shape
        
        q = self.q_proj(x).view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(B, S, self.n_heads, self.head_dim).transpose(1, 2)

        if layer_past is not None:
            # Concatenate past keys/values (k, v) for generation
            past_k, past_v = layer_past
            k = torch.cat((past_k, k), dim=2)
            v = torch.cat((past_v, v), dim=2)
        
        present = (k, v)
        
        # Scaled Dot-Product Attention
        attn_weights = torch.matmul(q, k.transpose(2, 3)) / (self.head_dim ** 0.5)
        
        # Causal Mask (ensures a token only sees tokens before it)
        full_S = k.size(2)
        if S > 1 or full_S > S: 
             mask = torch.ones(S, full_S, device=x.device).tril(diagonal=(full_S - S))
             attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))
        
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(B, S, D)
        return self.out_proj(attn_output), present

class TransformerEncoderLayerWithCache(nn.Module):
    """Standard GPT-2 Transformer Block, used in the Top Stack."""
    def __init__(self, d_model, n_heads, dim_feedforward=None, dropout=0.1):
        super().__init__()
        dim_feedforward = dim_feedforward or 4 * d_model
        self.attn = SelfAttentionWithCache(d_model, n_heads)
        self.norm1 = nn.LayerNorm(d_model) # ln_1
        self.dropout1 = nn.Dropout(dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward) # c_fc
        self.linear2 = nn.Linear(dim_feedforward, d_model) # c_proj
        self.norm2 = nn.LayerNorm(d_model) # ln_2
        self.dropout2 = nn.Dropout(dropout)
        self.act = nn.GELU(approximate='tanh') # GPT-2 uses GELU

    def forward(self, x, layer_past: Optional[Tuple[torch.Tensor]] = None):
        # GPT-2 Architecture: LayerNorm is applied *before* attention/MLP (pre-norm)
        # x = x + attn(ln1(x))
        norm_x = self.norm1(x)
        attn_output, present = self.attn(norm_x, layer_past)
        x = x + self.dropout1(attn_output)
        
        # x = x + mlp(ln2(x))
        norm_x2 = self.norm2(x)
        mlp_out = self.linear2(self.act(self.linear1(norm_x2)))
        x = x + self.dropout2(mlp_out)
        return x, present

class AdaptiveBlock(nn.Module):
    """
    Adaptive Transformer Block, used in the Bottom Stack for early exit opportunities.
    """
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.block = TransformerEncoderLayerWithCache(d_model, n_heads)
        # Small network to predict confidence/complexity of the current input
        self.confidence_predictor = nn.Sequential(
            nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(d_model, 1), nn.Sigmoid()
        )
    
    def forward(self, x, layer_past: Optional[Tuple[torch.Tensor]] = None):
        x_out, present = self.block(x, layer_past)
        # Compute confidence from the average token embedding
        if x_out.size(1) > 1:
            conf = self.confidence_predictor(x_out.transpose(1, 2)).mean(dim=0)
        else:
            conf = x_out.new_tensor([0.0]) # Placeholder for single-token generation
        return x_out, conf, present

# ==================== 4. ADAPTIVE LATTICE/HORIZON PROCESSORS (Simplified) ====================
class AdaptiveLatticeProcessor(nn.Module):
    def __init__(self, d_model, max_seq_len):
        super().__init__()
        self.analyzer = RecursiveDescentLatticeAnalyzer(max_seq_len)
        self.layer_processors = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model, nhead=8, batch_first=True) for _ in range(2) 
        ])
    
    def forward(self, x, horizon_targets=None):
        # Simplified: runs input through two additional processing layers
        h = x
        for processor in self.layer_processors:
            h = processor(h)
        return h

class RecursiveHorizonPredictor(nn.Module):
    def __init__(self, d_model, vocab_size, horizon=8):
        super().__init__()
        self.horizon = horizon
        self.vocab_size = vocab_size
        self.coarse_predictor = nn.Linear(d_model, vocab_size)

    def forward(self, h_sequence):
        # Placeholder: returns dummy logits for the horizon
        B = h_sequence.shape[0]
        logits = torch.zeros(B, self.horizon, self.vocab_size, device=h_sequence.device)
        confidence = torch.ones(B, self.horizon, device=h_sequence.device)
        return logits, confidence

# ==================== 5. HST v5 MODEL (The Student) ====================
class HSTv5(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len=1024, horizon=8,
                 early_exit_threshold=0.93, mode='chunk', chunk_size=128):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_bottom_layers = n_layers // 2 # 6 layers
        self.n_top_layers = n_layers - self.n_bottom_layers # 6 layers
        self.mode = mode
        
        # Shared Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len * chunk_size, d_model) # Extended position capacity

        # Core Architecture
        self.adaptive_bottom = nn.ModuleList([AdaptiveBlock(d_model, n_heads) for _ in range(self.n_bottom_layers)])
        self.top_stack = nn.ModuleList([TransformerEncoderLayerWithCache(d_model, n_heads) for _ in range(self.n_top_layers)])
        
        # Hierarchical Components
        self.lattice_core = AdaptiveLatticeProcessor(d_model, max_seq_len)
        self.horizon_predictor = RecursiveHorizonPredictor(d_model, vocab_size, horizon=horizon)
        self.chunk_encoder = ChunkEncoder(d_model, chunk_size)
        
        # Final Norm and Head
        self.ln_f = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

    def load_gpt2_weights(self, gpt2_model):
        """CRITICAL: Transfers all weights from the GPT-2 Teacher model."""
        print("\n=== TRANSFERRING GPT-2 WEIGHTS ===")
        
        # 1. Embeddings
        self.token_embedding.weight.data.copy_(gpt2_model.transformer.wte.weight.data)
        n_pos = min(self.pos_embedding.weight.shape[0], gpt2_model.transformer.wpe.weight.shape[0])
        self.pos_embedding.weight.data[:n_pos].copy_(gpt2_model.transformer.wpe.weight.data[:n_pos])
        print("-> Embeddings transferred.")
        
        gpt2_layers = gpt2_model.transformer.h
        
        # 2. Layers (Split 12 layers -> 6 Bottom + 6 Top)
        for i in range(self.n_bottom_layers):
            self._transfer_block(gpt2_layers[i], self.adaptive_bottom[i].block)
        print(f"-> Bottom Stack (Layers 0-{self.n_bottom_layers-1}) transferred.")

        for i in range(self.n_top_layers):
            self._transfer_block(gpt2_layers[self.n_bottom_layers + i], self.top_stack[i])
        print(f"-> Top Stack (Layers {self.n_bottom_layers}-{self.n_bottom_layers+self.n_top_layers-1}) transferred.")
            
        # 3. Final Norm and LM Head
        self.ln_f.weight.data.copy_(gpt2_model.transformer.ln_f.weight.data)
        self.ln_f.bias.data.copy_(gpt2_model.transformer.ln_f.bias.data)
        self.lm_head.weight.data.copy_(self.token_embedding.weight.data)
        print("-> Final Norm and LM Head synchronized.")
        
    def _transfer_block(self, src_block, tgt_block):
        # Attention Projections (Q, K, V are combined in GPT-2's c_attn)
        w = src_block.attn.c_attn.weight.t() 
        b = src_block.attn.c_attn.bias 
        qw, kw, vw = w.split(768, dim=0)
        
        tgt_block.attn.q_proj.weight.data.copy_(qw)
        tgt_block.attn.k_proj.weight.data.copy_(kw)
        tgt_block.attn.v_proj.weight.data.copy_(vw)
        
        # Attention Output Projection
        tgt_block.attn.out_proj.weight.data.copy_(src_block.attn.c_proj.weight.t())
        
        # Layer Norms
        tgt_block.norm1.weight.data.copy_(src_block.ln_1.weight.data)
        tgt_block.norm1.bias.data.copy_(src_block.ln_1.bias.data)
        tgt_block.norm2.weight.data.copy_(src_block.ln_2.weight.data)
        tgt_block.norm2.bias.data.copy_(src_block.ln_2.bias.data)
        
        # MLP Layers
        tgt_block.linear1.weight.data.copy_(src_block.mlp.c_fc.weight.t())
        tgt_block.linear1.bias.data.copy_(src_block.mlp.c_fc.bias.data)
        tgt_block.linear2.weight.data.copy_(src_block.mlp.c_proj.weight.t())
        tgt_block.linear2.bias.data.copy_(src_block.mlp.c_proj.bias.data)

    def forward(self, input_ids: torch.Tensor, past_key_values: Optional[List[Tuple[torch.Tensor]]] = None, labels: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        
        B, S = input_ids.shape
        # Determine full sequence length including cache
        full_S = S + (past_key_values[0][0].size(2) if past_key_values else 0)
        
        # 1. Embeddings
        pos_ids = torch.arange(full_S - S, full_S, dtype=torch.long, device=input_ids.device)
        token_emb = self.token_embedding(input_ids)
        pos_emb = self.pos_embedding(pos_ids)
        h = token_emb + pos_emb
        
        new_past_key_values = []
        
        # 2. Bottom Stack (Adaptive Blocks)
        for i, block in enumerate(self.adaptive_bottom):
            past = past_key_values[i] if past_key_values else None
            h, conf, present = block(h, past)
            new_past_key_values.append(present)

        # 3. Lattice Core (Global Context Integration)
        h_latent = self.lattice_core(h)
        h = h + h_latent # Residual connection

        # 4. Top Stack (Standard Transformer Blocks)
        for i, block in enumerate(self.top_stack):
            past = past_key_values[self.n_bottom_layers + i] if past_key_values else None
            h, present = block(h, past)
            new_past_key_values.append(present)

        # 5. Final Layer Norm and LM Head
        h = self.ln_f(h)
        logits = self.lm_head(h)
        
        return {'logits': logits, 'past_key_values': new_past_key_values}

# ==================== 6. ADAPTATION AND GENERATION FUNCTIONS ====================

def train_adaptation(model, gpt2_model, tokenizer):
    """
    Performs the Knowledge Distillation (KD) adaptation.
    
    """
    if TRAIN_STEPS == 0:
        print("Skipping training. Only weights transferred.")
        return

    print(f"Starting minimal adaptation training ({TRAIN_STEPS} step) with Knowledge Distillation...")
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    
    # Use PROMPT_TEXT as the *only* data input
    tokens = tokenizer.encode(PROMPT_TEXT, return_tensors='pt').to(DEVICE).repeat(BATCH_SIZE, 1)

    for step in range(TRAIN_STEPS):
        optimizer.zero_grad()
        
        # 1. Student (HST) Forward Pass
        outputs = model(tokens)
        logits = outputs['logits']
        
        # 2. Student Cross-Entropy Loss (Standard next-token prediction)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = tokens[..., 1:].contiguous()
        ce_loss = F.cross_entropy(shift_logits.view(-1, VOCAB_SIZE), shift_labels.view(-1))
        
        # 3. Teacher (GPT-2) Output (Soft Targets)
        gpt2_model.eval()
        with torch.no_grad():
            gpt2_outputs = gpt2_model(tokens, labels=None)
            gpt2_logits = gpt2_outputs.logits
            
        # 4. Knowledge Distillation Loss (KL Divergence between soft targets)
        soft_targets = F.softmax(gpt2_logits[..., :-1, :] / KD_TEMPERATURE, dim=-1)
        soft_predictions = F.log_softmax(shift_logits / KD_TEMPERATURE, dim=-1)
        
        kd_loss = F.kl_div(
            soft_predictions.view(-1, VOCAB_SIZE), 
            soft_targets.view(-1, VOCAB_SIZE),
            reduction='batchmean'
        ) * (KD_TEMPERATURE * KD_TEMPERATURE)
        
        # 5. Combined Loss
        loss = (1 - KD_ALPHA) * ce_loss + KD_ALPHA * kd_loss
        
        loss.backward()
        optimizer.step()
        
        print(f"Step {step+1}/{TRAIN_STEPS} | Loss: {loss.item():.4f} (CE: {ce_loss.item():.4f}, KD: {kd_loss.item():.4f})")

def generate_text_chunk_mode(model, tokenizer, prompt, max_gen_tokens, chunk_size, temperature, top_p):
    """
    Generates text in Chunk Mode using KV-caching and Top-P sampling 
    to ensure long, coherent, non-repetitive output.
    """
    model.eval()
    start_time = time.time()
    generated_tokens = tokenizer.encode(prompt, return_tensors='pt').to(DEVICE)
    
    with torch.no_grad():
        for _ in range(max_gen_tokens // chunk_size):
            
            # Start/continue generation from the last MAX_SEQ_LEN tokens
            input_ids = generated_tokens[:, -MAX_SEQ_LEN:]
            
            # Initial forward pass to get the first set of cache/past_key_values
            outputs = model(input_ids)
            past_key_values = outputs['past_key_values']
            next_token_logits = outputs['logits'][:, -1, :] 
            
            # Generation loop for the chunk (token-by-token)
            for __ in range(chunk_size):
                
                # Apply Temperature and Top-P (Nucleus) sampling
                probabilities = F.softmax(next_token_logits / temperature, dim=-1)
                
                sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                
                # Mask tokens with cumulative probability above p
                mask = cumulative_probs > top_p
                # Shift mask to keep the first token (largest probability)
                mask[:, 1:] = mask[:, :-1].clone()
                mask[:, 0] = False
                
                # Set probabilities of masked tokens to zero and re-normalize
                probabilities[0, sorted_indices[0, mask.squeeze()]] = 0.0
                probabilities = probabilities / probabilities.sum(dim=-1, keepdim=True)

                # Sample the next token
                next_token = torch.multinomial(probabilities, num_samples=1)

                # Append to the generated sequence
                generated_tokens = torch.cat([generated_tokens, next_token], dim=-1)

                # Prepare input for the next token (only the newly generated token)
                next_input = next_token

                # Update cache in place for efficient single-token generation
                outputs = model(next_input, past_key_values=past_key_values)
                past_key_values = outputs['past_key_values']
                next_token_logits = outputs['logits'][:, -1, :]

            # Print progress and a snippet
            current_tokens = generated_tokens.shape[1]
            print(f" ¬†-> Generating Chunk {_+1}/{(max_gen_tokens // chunk_size)} ({current_tokens} tokens)...")
            
    end_time = time.time()
    total_time = end_time - start_time
    total_generated = generated_tokens.shape[1] - tokenizer.encode(prompt, return_tensors='pt').shape[1]
    
    text = tokenizer.decode(generated_tokens.squeeze(), skip_special_tokens=True)
    
    print("======================================================================")
    print(f"Total time: {total_time:.2f}s | Avg TPS: {total_generated / total_time:.2f}")
    return text

def main():
    print("Loading official GPT-2 (Teacher)...")
    # Load the full GPT-2 model (Teacher)
    gpt2_source = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token
    
    print("Initializing HST v5 (Student) with Strict Architecture...")
    # Initialize the HSTv5 (Student)
    model = HSTv5(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS, MAX_SEQ_LEN).to(DEVICE)
    
    # 1. Full Weight Transfer
    model.load_gpt2_weights(gpt2_source)
    
    # 2. Minimal Zero-Data Adaptation using KD
    train_adaptation(model, gpt2_source, tokenizer) 
    
    # Release the Teacher model to free up GPU memory
    del gpt2_source 
    torch.cuda.empty_cache()

    print(f"\n======================================================================")
    print(f"HST-v5 CHUNK MODE GENERATION ({MAX_GEN_TOKENS} tokens) - FINAL RUN")
    print(f"======================================================================")
    
    # 3. Long-Form Generation
    text_output = generate_text_chunk_mode(
        model, tokenizer, PROMPT_TEXT, MAX_GEN_TOKENS, CHUNK_SIZE, TEMPERATURE, TOP_P
    )
    
    # 4. Final Output
    with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
        f.write(text_output)

    print(f"Saved to {OUTPUT_FILENAME}")
    print("\nüìù **Generated Text Snippet (First 500 Characters):**")
    print("--------------------------------------------------")
    print(text_output[:500])
    print("--------------------------------------------------")

if __name__ == '__main__':
    main()