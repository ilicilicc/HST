```python
# -*- coding: utf-8 -*-
"""
HST Hybrid - Advanced Hybrid Model for Ultra-High TPS on T4 GPU
Integrates best from v5, v6, XX: CompleteLatticeCore, Speculative Gen, Optimizations
Targets: >2000 TPS with FP16, Torch-TensorRT, INT8, Larger Chunks
"""

# ==================== SETUP AND IMPORTS ====================
import os
import sys
import gc
import numpy as np
from typing import Dict, Optional, Tuple, List
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup
from datasets import load_dataset
from torch.nn.attention import sdpa_kernel, SDPBackend
try:
    import bitsandbytes as bnb
    USE_BNB = True
except ImportError:
    USE_BNB = False
    print("bitsandbytes not installed. Skipping advanced quantization.")
try:
    import torch_tensorrt  # For TensorRT optimization
except ImportError:
    print("torch_tensorrt not installed. Install for better perf.")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.set_float32_matmul_precision('high')

# ==================== HYPERPARAMETERS ====================
D_MODEL = 768
N_HEADS = 12
N_LAYERS = 16
MAX_SEQ_LEN = 1024  # Increased
HORIZON = 16  # Increased for better speculation
BATCH_SIZE = 8
GRADIENT_ACCUMULATION_STEPS = 4
MAX_TRAINING_STEPS = 1000
INITIAL_LR = 2e-4
WARMUP_STEPS = 200
MODE = 'chunk'
CHUNK_SIZE = 256  # Larger for fewer passes
SAVE_CHECKPOINT_STEPS = 100

# Generation Config
MAX_GEN_TOKENS = 50048
TEMPERATURE = 1.0
TOP_K = 50
MAX_CACHE_SIZE = 2048  # For pruning

save_dir = './hst_hybrid_checkpoints'
os.makedirs(save_dir, exist_ok=True)
OUTPUT_FILENAME = "hst_hybrid_story_50k_tokens_ULTRAFAST.txt"

KVCache = Optional[List[Tuple[torch.Tensor, torch.Tensor]]]

def get_sdpa_backend():
    if torch.cuda.is_available():
        return SDPBackend.FLASH_ATTENTION
    return SDPBackend.MATH

def print_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")

def prune_cache(cache: KVCache, max_size: int = MAX_CACHE_SIZE) -> KVCache:
    if not cache or cache[0][0].size(2) <= max_size:
        return cache
    pruned_cache = []
    for k, v in cache:
        pruned_k = k[:, :, -max_size:, :]
        pruned_v = v[:, :, -max_size:, :]
        pruned_cache.append((pruned_k, pruned_v))
    return pruned_cache

# ==========================================================
# 1. ADVANCED LATTICE COMPONENTS (From XX)
# ==========================================================
class RecursiveDescentLatticeAnalyzer(nn.Module):
    def __init__(self, max_seq_len=8192):
        super().__init__()
        spine_list = self._generate_spine_list(max_seq_len)
        self.register_buffer('spine', torch.tensor(spine_list, dtype=torch.long))
        self.descent_paths = self._compute_descent_paths()
        self.layer_weights = nn.Parameter(torch.ones(10))

    def _generate_spine_list(self, max_len):
        spine = [0, 2, 4]
        while True:
            next_val = 2 * spine[-1] + 2 * spine[-2] + 2 * spine[-3]
            if next_val >= max_len:
                break
            spine.append(next_val)
        return spine

    def _nearest_spine(self, pos):
        return self.spine[(self.spine.float() - pos).abs().argmin()]

    def _find_parent(self, pos):
        if pos == 0:
            return 0
        parent_approx = pos / 2.414
        return self._nearest_spine(parent_approx).item()

    def _compute_descent_paths(self):
        paths = {}
        for pos_tensor in self.spine:
            pos = pos_tensor.item()
            path = []
            current = pos
            layer = 0
            while current > 0 and layer < 10:
                parent = self._find_parent(current)
                path.append((layer, parent))
                if current == parent:
                    break
                current = parent
                layer += 1
            paths[pos] = path
        return paths

    def compute_predictive_field(self, pos, target_offset):
        spine_distance = int(np.log2(target_offset + 1))
        layer_importance = torch.zeros(10, device=self.layer_weights.device)
        if spine_distance > 5:
            layer_importance[0:3] = torch.tensor([1.0, 0.8, 0.5])
        elif spine_distance > 2:
            layer_importance[1:5] = torch.tensor([0.5, 1.0, 0.8, 0.3])
        else:
            layer_importance[3:7] = torch.tensor([0.3, 0.8, 1.0, 0.8])
        layer_importance = layer_importance * torch.sigmoid(self.layer_weights)
        return layer_importance

class FullLatticeFieldAnalyzer(nn.Module):
    def __init__(self, max_seq_len=8192):
        super().__init__()
        spine = [0, 2, 4]
        while True:
            next_val = 2*spine[-1] + 2*spine[-2] + 2*spine[-3]
            if next_val >= max_seq_len:
                break
            spine.append(next_val)
        self.register_buffer('spine', torch.tensor(spine, dtype=torch.long))
        self.max_depth = self._compute_max_depth()
        self.lattice_structure = {}
        for pos in spine:
            if pos < max_seq_len:
                self.lattice_structure[pos] = self._analyze_position(pos)
        self._non_spine_cache = {}

    def _compute_max_depth(self):
        return len(self.spine)

    def get_structure(self, pos: int):
        if pos in self.lattice_structure:
            return self.lattice_structure[pos]
        if pos in self._non_spine_cache:
            return self._non_spine_cache[pos]
        structure = self._analyze_non_spine(pos)
        self._non_spine_cache[pos] = structure
        return structure

    def _analyze_position(self, pos):
        levels = {0: [pos]}
        visited = {pos}
        current_level = [pos]
        level = 0
        while current_level and level < 10:
            next_level = set()
            for node in current_level:
                ancestors = self._get_immediate_ancestors(node)
                for anc in ancestors:
                    if anc not in visited and anc >= 0:
                        visited.add(anc)
                        next_level.add(anc)
            current_level = list(next_level)
            level += 1
            if current_level:
                levels[level] = current_level.copy()
        max_depth = max(levels.keys()) if levels else 0
        path_counts = self._compute_path_counts(pos, levels, max_depth)
        return {
            'levels': levels,
            'path_counts': path_counts,
            'total_ancestors': len(visited) - 1,
            'max_depth': max_depth
        }

    def _get_immediate_ancestors(self, pos):
        try:
            idx = (self.spine == pos).nonzero(as_tuple=True)[0].item()
            if idx >= 3:
                return [
                    self.spine[idx-1].item(),
                    self.spine[idx-2].item(),
                    self.spine[idx-3].item()
                ]
        except:
            pass
        return []

    def _analyze_non_spine(self, pos):
        left_spine = self.spine[self.spine < pos]
        ancestors = []
        if len(left_spine) > 0:
            ancestors.append(left_spine[-1].item())
        return {
            'levels': {0: [pos], 1: ancestors},
            'path_counts': {anc: 1 for anc in ancestors},
            'total_ancestors': len(ancestors),
            'max_depth': 1
        }

    def _compute_path_counts(self, pos, levels, max_depth):
        path_counts = {pos: 1}
        for level in sorted(levels.keys(), reverse=True):
            for node in levels[level]:
                if node == pos: continue
                count = 0
                if level == max_depth:
                    path_counts[node] = 1
                    continue
                for child in levels.get(level + 1, []):
                    if node in self._get_immediate_ancestors(child):
                        count += path_counts.get(child, 0)
                if level != 0:
                    path_counts[node] = count
        path_counts.pop(pos, None)
        return path_counts

class MultiLevelLatticeProcessor(nn.Module):
    def __init__(self, d_model, max_seq_len):
        super().__init__()
        self.d_model = d_model
        self.analyzer = FullLatticeFieldAnalyzer(max_seq_len)
        self.level_transforms = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.LayerNorm(d_model),
                nn.GELU(),
                nn.Linear(d_model, d_model)
            ) for _ in range(10)
        ])
        self.level_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=4,
            batch_first=True
        )
        self.fusion = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.LayerNorm(d_model)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, S, D = x.shape
        spine = self.analyzer.spine
        relevant_spine = spine[spine < S].tolist()
        updates = {}
        for spine_pos in relevant_spine:
            if spine_pos < 3: continue
            structure = self.analyzer.get_structure(spine_pos)
            if structure is None: continue
            level_features = []
            for level in range(structure['max_depth'] + 1):
                if level == 0: continue
                if level not in structure['levels']: continue
                level_nodes = structure['levels'][level]
                level_h = []
                total_weight = 0.0
                for node in level_nodes:
                    if node < S:
                        weight = structure['path_counts'].get(node, 1)
                        level_h.append(x[:, node, :] * weight)
                        total_weight += weight
                if level_h and total_weight > 0:
                    level_feat = torch.stack(level_h, dim=1).sum(dim=1) / total_weight
                    level_feat = self.level_transforms[level](level_feat)
                    level_features.append(level_feat)
            if not level_features: continue
            level_stack = torch.stack(level_features, dim=1)
            query = x[:, spine_pos:spine_pos+1, :]
            attended, _ = self.level_attention(query, level_stack, level_stack)
            combined = torch.cat([attended.squeeze(1), x[:, spine_pos, :]], dim=-1)
            updates[spine_pos] = self.fusion(combined)
        if not updates:
            return x
        sorted_positions = sorted(updates.keys())
        output_slices = []
        last_pos = 0
        for pos in sorted_positions:
            if pos > last_pos:
                output_slices.append(x[:, last_pos:pos, :])
            output_slices.append(updates[pos].unsqueeze(1))
            last_pos = pos + 1
        if last_pos < S:
            output_slices.append(x[:, last_pos:S, :])
        return torch.cat(output_slices, dim=1)

class PathWeightedLatticeCore(nn.Module):
    def __init__(self, d_model, max_seq_len):
        super().__init__()
        self.d_model = d_model
        self.analyzer = FullLatticeFieldAnalyzer(max_seq_len)
        self.path_weight_net = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Softplus()
        )
        self.message_fn = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.LayerNorm(d_model),
            nn.GELU()
        )
        self.aggregate_fn = nn.GRU(d_model, d_model, batch_first=True)
        self.update_gate = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.Sigmoid()
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, S, D = x.shape
        spine = self.analyzer.spine
        relevant_spine = spine[spine < S].tolist()
        updates = {}
        for spine_pos in relevant_spine:
            if spine_pos < 3: continue
            structure = self.analyzer.get_structure(spine_pos)
            if structure is None or structure['total_ancestors'] == 0: continue
            all_ancestors = []
            path_counts = []
            for level in structure['levels']:
                if level > 0:
                    for anc in structure['levels'][level]:
                        if anc < S:
                            all_ancestors.append(anc)
                            path_counts.append(structure['path_counts'].get(anc, 1))
            if not all_ancestors: continue
            path_count_tensor = torch.tensor(path_counts, device=x.device).view(-1, 1).float()
            path_weights_tensor = self.path_weight_net(path_count_tensor).squeeze()
            messages = []
            for ancestor_pos in all_ancestors:
                h_anc = x[:, ancestor_pos, :]
                h_curr = x[:, spine_pos, :]
                msg = self.message_fn(torch.cat([h_anc, h_curr], dim=-1))
                messages.append(msg)
            msg_stack = torch.stack(messages, dim=1)
            if path_weights_tensor.dim() == 0:
                weights_tensor = path_weights_tensor.view(1, 1, 1).expand(B, -1, D)
            else:
                weights_tensor = path_weights_tensor.view(1, -1, 1).expand(B, -1, D)
            weighted_msgs = msg_stack * weights_tensor
            aggregated, _ = self.aggregate_fn(weighted_msgs)
            aggregated = aggregated[:, -1, :]
            gate = self.update_gate(torch.cat([aggregated, x[:, spine_pos, :]], dim=-1))
            updates[spine_pos] = gate * aggregated + (1 - gate) * x[:, spine_pos, :]
        if not updates:
            return x
        sorted_positions = sorted(updates.keys())
        output_slices = []
        last_pos = 0
        for pos in sorted_positions:
            if pos > last_pos:
                output_slices.append(x[:, last_pos:pos, :])
            output_slices.append(updates[pos].unsqueeze(1))
            last_pos = pos + 1
        if last_pos < S:
            output_slices.append(x[:, last_pos:S, :])
        return torch.cat(output_slices, dim=1)

class CompleteLatticeCore(nn.Module):
    def __init__(self, d_model, max_seq_len):
        super().__init__()
        self.multi_level = MultiLevelLatticeProcessor(d_model, max_seq_len)
        self.path_weighted = PathWeightedLatticeCore(d_model, max_seq_len)
        self.meta_fusion = nn.Sequential(
            nn.Linear(d_model * 3, d_model * 2),
            nn.LayerNorm(d_model * 2),
            nn.GELU(),
            nn.Linear(d_model * 2, d_model)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        h_multi = self.multi_level(x)
        h_path = self.path_weighted(x)
        h_combined = torch.cat([x, h_multi, h_path], dim=-1)
        h_out = self.meta_fusion(h_combined)
        return h_out

# ==========================================================
# 2. CHUNK ENCODER/DECODER (SDPA + FP16)
# ==========================================================
class ChunkEncoder(nn.Module):
    def __init__(self, d_model, chunk_size=256, n_heads=12, n_layers=2):
        super().__init__()
        self.chunk_size = chunk_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        encoder_layer = nn.TransformerEncoderLayer(
            d_model, n_heads, d_model * 4, batch_first=True,
            norm_first=True,
        )
        self.local_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.pooling_query = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)
        self.pooling_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)

    def forward(self, token_embeddings):
        B, total_tokens, D = token_embeddings.shape
        num_chunks = total_tokens // self.chunk_size
        if num_chunks == 0:
            return token_embeddings.new_zeros(B, 0, D)
        tokens_to_use = num_chunks * self.chunk_size
        chunks = token_embeddings[:, :tokens_to_use, :].view(B * num_chunks, self.chunk_size, D)
        encoded_tokens = self.local_encoder(chunks)
        query = self.pooling_query.expand(B * num_chunks, -1, -1)
        pooled, _ = self.pooling_attn(query, encoded_tokens, encoded_tokens)
        return pooled.view(B, num_chunks, D)

class ChunkDecoder(nn.Module):
    def __init__(self, d_model, vocab_size, chunk_size=256, n_heads=12, n_layers=2):
        super().__init__()
        self.chunk_size = chunk_size
        self.pos_embedding = nn.Embedding(chunk_size, d_model)
        decoder_layer = nn.TransformerDecoderLayer(
            d_model, n_heads, d_model * 4, batch_first=True,
            norm_first=True,
        )
        self.local_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, chunk_embeddings, target_token_embeddings):
        B, num_chunks, D = chunk_embeddings.shape
        seq_len = min(target_token_embeddings.size(1), num_chunks * self.chunk_size)
        target_token_embeddings = target_token_embeddings[:, :seq_len, :]
        pos = torch.arange(0, self.chunk_size, device=target_token_embeddings.device).unsqueeze(0)
        pos_emb = self.pos_embedding(pos).repeat(B * num_chunks, 1, 1)
        tgt = target_token_embeddings.view(B * num_chunks, self.chunk_size, D) + pos_emb
        memory = chunk_embeddings.view(B * num_chunks, 1, D).repeat(1, self.chunk_size, 1)
        causal_mask = nn.Transformer.generate_square_subsequent_mask(self.chunk_size).to(tgt.device)
        refined = self.local_decoder(tgt, memory, tgt_mask=causal_mask)
        refined = refined.view(B, seq_len, D)
        return self.lm_head(refined)

# ==========================================================
# 3. SDPA-BASED ATTENTION
# ==========================================================
class SDPAWithCache(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=False)

    def forward(self, x, layer_past=None, use_sdpa=True):
        B, S, D = x.shape
        q = self.q_proj(x).view(B, S, self.n_heads, -1).transpose(1, 2)
        k = self.k_proj(x).view(B, S, self.n_heads, -1).transpose(1, 2)
        v = self.v_proj(x).view(B, S, self.n_heads, -1).transpose(1, 2)
        if layer_past is not None:
            past_k, past_v = layer_past
            k = torch.cat((past_k, k), dim=2)
            v = torch.cat((past_v, v), dim=2)
        present = (k, v)
        if use_sdpa:
            attn_output = F.scaled_dot_product_attention(
                q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True,
                scale=None, backend=get_sdpa_backend()
            )
        else:
            attn_output = F.multi_head_attention_forward(
                q, k, v, self.out_proj.weight, None, None, None, None, None, None
            )[0]
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(attn_output)
        return out, present

# ==========================================================
# 4. RECURSIVE HORIZON PREDICTOR (Advanced)
# ==========================================================
class RecursiveHorizonPredictor(nn.Module):
    def __init__(self, d_model, vocab_size, horizon=16):
        super().__init__()
        self.horizon = horizon
        self.coarse_predictor = nn.Linear(d_model, vocab_size)
        self.medium_predictor = nn.Linear(d_model + d_model, vocab_size)
        self.fine_predictor = nn.Linear(d_model + d_model, vocab_size)
        self.lattice_embeddings = nn.Embedding(20, d_model)
        self.projection = nn.Linear(vocab_size, d_model)

    def forward(self, h_sequence):
        B, S, D = h_sequence.shape
        h_t = h_sequence[:, -1, :]
        coarse_offsets = [4, 10]
        coarse_preds = {}
        for offset in coarse_offsets:
            offset_emb = self.lattice_embeddings(torch.tensor([min(offset - 1, 19)], device=h_t.device))
            h_augmented = h_t + offset_emb.squeeze(0)
            pred = self.coarse_predictor(h_augmented)
            coarse_preds[offset] = pred
        medium_offsets = [2, 6]
        medium_preds = {}
        for offset in medium_offsets:
            left_coarse = coarse_preds[4]
            right_coarse = coarse_preds[10]
            alpha = (offset - 4) / (10 - 4)
            coarse_interp = self.projection(alpha * left_coarse + (1 - alpha) * right_coarse)
            h_interpolated = torch.cat([h_t, coarse_interp], dim=-1)
            pred = self.medium_predictor(h_interpolated)
            medium_preds[offset] = pred
        fine_offsets = [1, 3, 5]
        fine_preds = {}
        for offset in fine_offsets:
            left_med = medium_preds[2]
            right_med = medium_preds[6]
            alpha = (offset - 2) / (6 - 2)
            medium_interp = self.projection(alpha * left_med + (1-alpha) * right_med)
            h_interpolated = torch.cat([h_t, medium_interp], dim=-1)
            pred = self.fine_predictor(h_interpolated)
            fine_preds[offset] = pred
        all_preds = {**coarse_preds, **medium_preds, **fine_preds}
        logits_list = [all_preds.get(i, torch.zeros(B, self.coarse_predictor.out_features, device=h_t.device)) for i in range(1, self.horizon + 1)]
        logits = torch.stack(logits_list, dim=1)
        confidence = torch.ones(B, self.horizon, device=h_t.device)
        return logits, confidence

# ==========================================================
# 5. HST HYBRID MODEL
# ==========================================================
class HSTHybrid(nn.Module):
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            std = 0.02
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)

    def __init__(self, vocab_size, d_model, n_heads, n_layers, max_seq_len=1024, horizon=16, mode='chunk', chunk_size=256):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.horizon = horizon
        self.chunk_size = chunk_size
        self.max_num_chunks = max_seq_len // chunk_size
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(self.max_num_chunks, d_model)
        self.chunk_encoder = ChunkEncoder(d_model, chunk_size, n_heads, n_layers=2)
        self.chunk_decoder = ChunkDecoder(d_model, vocab_size, chunk_size, n_heads, n_layers=2)
        self.lattice_core = CompleteLatticeCore(d_model, max_seq_len)
        self.horizon_predictor = RecursiveHorizonPredictor(d_model, vocab_size, horizon)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        self.ln_f = nn.LayerNorm(d_model)
        self.apply(self._init_weights)
        self.lm_head.weight = self.token_embedding.weight

    def forward(self, input_ids):
        return self.forward_chunk(input_ids)

    def forward_chunk(self, input_ids):
        B, total_tokens = input_ids.shape
        device = input_ids.device
        x = self.token_embedding(input_ids)
        chunk_emb = self.chunk_encoder(x)
        B, num_chunks, D = chunk_emb.shape
        if num_chunks == 0:
            zero_logits = torch.zeros(B, total_tokens, self.vocab_size, device=device, dtype=torch.float16)
            zero_horizon = torch.zeros(B, self.horizon, self.vocab_size, device=device, dtype=torch.float16)
            return {
                'logits': zero_logits, 'horizon_logits': zero_horizon,
                'confidence': torch.zeros(B, self.horizon, device=device),
                'hidden_states': chunk_emb, 'bottom_depth': 0, 'cache': None
            }
        chunk_positions = torch.arange(0, num_chunks, dtype=torch.long, device=device).clamp(max=self.pos_embedding.num_embeddings - 1)
        h_in = chunk_emb + self.pos_embedding(chunk_positions)
        h_lattice_out = self.lattice_core(h_in)
        logits = self.chunk_decoder(h_lattice_out, x)
        logits_horizon, confidence = self.horizon_predictor(h_lattice_out)
        return {
            'logits': logits,
            'horizon_logits': logits_horizon,
            'confidence': confidence,
            'hidden_states': h_lattice_out,
            'bottom_depth': 0,
            'cache': None
        }

    def quantize_int8(self):
        if USE_BNB:
            self.token_embedding = bnb.nn.Embedding(self.vocab_size, self.d_model)
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    module.weight = bnb.nn.Params4bit(module.weight.data, requires_grad=False)
        return self

# ==========================================================
# 6. LOSS FUNCTION
# ==========================================================
def compute_loss(output, targets, horizon=16, gamma=0.95, pad_id=None, n_layers=16):
    if pad_id is None:
        raise ValueError("pad_id must be provided to compute_loss.")
    logits = output['logits']
    B, S = targets.shape
    V = logits.size(-1)
    logits_to_use = logits[:, :-1].reshape(-1, V)
    targets_to_use = targets[:, 1:].reshape(-1)
    min_len = min(logits_to_use.size(0), targets_to_use.size(0))
    lm_loss = F.cross_entropy(logits_to_use[:min_len], targets_to_use[:min_len], ignore_index=pad_id)
    total_loss = lm_loss
    if 'horizon_logits' in output:
        horizon_logits = output['horizon_logits']
        S_chunks = output['hidden_states'].size(1)
        for k in range(1, horizon + 1):
            if S_chunks * CHUNK_SIZE + k >= S:
                break
            target_chunk_idx = S_chunks - 1 + k
            if target_chunk_idx * CHUNK_SIZE < S:
                h_target_token_idx = target_chunk_idx * CHUNK_SIZE
                h_logits_k = horizon_logits[:, k-1, :]
                h_targets_k = targets[:, h_target_token_idx]
                total_loss += (gamma ** k) * F.cross_entropy(h_logits_k, h_targets_k, ignore_index=pad_id)
    return total_loss

# ==========================================================
# 7. ULTRA-FAST SPECULATIVE GENERATION
# ==========================================================
@torch.no_grad()
def generate_speculative(model, tokenizer, prompt, max_new_tokens, chunk_size=CHUNK_SIZE, max_context_len=MAX_SEQ_LEN, temperature=TEMPERATURE, top_k=TOP_K):
    model.eval()
    device = next(model.parameters()).device
    model = model.to(torch.float16)
    compiled_forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)
    input_ids = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=max_context_len).input_ids.to(device)
    B, S_initial = input_ids.shape
    current_ids = input_ids
    total_generated = 0
    start_time = time.time()
    num_chunks_to_generate = (max_new_tokens + chunk_size - 1) // chunk_size
    print(f"Target: {max_new_tokens} tokens ({num_chunks_to_generate} chunks of {chunk_size}). Compiled & FP16 enabled.")
    for chunk_step in range(num_chunks_to_generate):
        if (chunk_step + 1) % 20 == 0:
            print(f" -> Generating Chunk {chunk_step + 1}/{num_chunks_to_generate} ({total_generated} tokens so far)...")
            torch.cuda.empty_cache()
        context_ids = current_ids[:, -max_context_len:]
        S_context = context_ids.size(1)
        padding_needed = chunk_size - (S_context % chunk_size)
        context_ids_padded = F.pad(context_ids, (0, padding_needed), value=tokenizer.pad_token_id) if padding_needed > 0 else context_ids
        placeholder_block = torch.full((B, chunk_size), tokenizer.pad_token_id, dtype=torch.long, device=device)
        input_for_pass = torch.cat([context_ids_padded, placeholder_block], dim=1)
        with autocast(dtype=torch.float16):
            output = compiled_forward(input_for_pass)
        logits = output['logits']
        new_chunk_logits = logits[:, S_context:, :]
        new_chunk_ids = []
        for i in range(chunk_size):
            logit_i = new_chunk_logits[0, i, :].float() / temperature
            if top_k > 0:
                v, _ = torch.topk(logit_i, top_k)
                logit_i[logit_i < v[-1]] = -float('Inf')
            probs = F.softmax(logit_i, dim=-1)
            sampled_id = torch.multinomial(probs, 1).item()
            new_chunk_ids.append(sampled_id)
        new_chunk_tensor = torch.tensor(new_chunk_ids[:min(chunk_size, max_new_tokens - total_generated)], dtype=torch.long, device=device).unsqueeze(0)
        current_ids = torch.cat([current_ids, new_chunk_tensor], dim=1)
        total_generated += new_chunk_tensor.size(1)
        if total_generated >= max_new_tokens:
            break
    end_time = time.time()
    total_time = end_time - start_time
    avg_tps = total_generated / total_time
    output_ids = current_ids[0].tolist()
    text_output = tokenizer.decode(output_ids, skip_special_tokens=True)
    stats = {
        'tokens_generated': total_generated,
        'total_time': total_time,
        'average_tps': avg_tps,
    }
    return text_output, stats

# ==================== MAIN EXECUTION ====================
print("\n[1/7] Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
VOCAB_SIZE = len(tokenizer)
PAD_ID = tokenizer.pad_token_id

print("[2/7] Building HST Hybrid model...")
model = HSTHybrid(
    vocab_size=VOCAB_SIZE, d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,
    max_seq_len=MAX_SEQ_LEN, horizon=HORIZON, mode=MODE, chunk_size=CHUNK_SIZE
).to(device)

model.quantize_int8()

total_params = sum(p.numel() for p in model.parameters())
print(f"Model: {total_params/1e6:.1f}M params (INT8 quantized)")
print_memory()

print("\n[3/7] Setting up optimizer...")
optimizer = torch.optim.AdamW(model.parameters(), lr=INITIAL_LR, weight_decay=0.01)
scaler = GradScaler()
scheduler = get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, MAX_TRAINING_STEPS)

latest_checkpoint = os.path.join(save_dir, 'hst_hybrid_interrupted_latest.pt')
if os.path.exists(latest_checkpoint):
    print(f"Loading checkpoint from: {latest_checkpoint}")
    checkpoint = torch.load(latest_checkpoint, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    start_step = checkpoint['step']
    print(f"Resuming from Step {start_step} (Loss: {checkpoint['current_loss']:.4f})")
else:
    start_step = 0
    print("Starting from scratch.")

print("[4/7] Loading dataset...")
dataset = load_dataset("HuggingFaceFW/fineweb-edu", "sample-10BT", split="train", streaming=True)

def tokenize_and_chunk(ex):
    if 'text' not in ex:
        return {'input_ids': []}
    tokenized = tokenizer(
        ex["text"],
        truncation=True,
        max_length=MAX_SEQ_LEN,
        padding='max_length',
        return_overflowing_tokens=False,
        return_attention_mask=False
    )
    return {"input_ids": tokenized["input_ids"]}

stream = dataset.map(tokenize_and_chunk, remove_columns=dataset.column_names).filter(lambda x: len(x['input_ids']) == MAX_SEQ_LEN)
collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
loader = DataLoader(stream, batch_size=BATCH_SIZE, collate_fn=collator, pin_memory=True, num_workers=2)

print(f"[5/7] Training (Max Steps: {MAX_TRAINING_STEPS})...\n")
model.train()
step = start_step
grad_acc_step = 0
current_loss = 0.0

try:
    for batch in loader:
        if step >= MAX_TRAINING_STEPS:
            break
        ids = batch["input_ids"].to(device, non_blocking=True)
        with autocast(dtype=torch.float16):
            out = model(ids)
            loss = compute_loss(out, ids, horizon=HORIZON, pad_id=PAD_ID, n_layers=N_LAYERS)
            loss = loss / GRADIENT_ACCUMULATION_STEPS
        scaler.scale(loss).backward()
        grad_acc_step += 1
        current_loss += loss.item()
        if grad_acc_step % GRADIENT_ACCUMULATION_STEPS == 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad()
            current_loss_print = current_loss * GRADIENT_ACCUMULATION_STEPS
            if step % 10 == 0 or step == start_step:
                print(f"Step {step:6d} | LR {optimizer.param_groups[0]['lr']:.2e} | Loss {current_loss_print:.4f} | ", end="")
                print_memory()
            step += 1
            current_loss = 0.0
            if step % SAVE_CHECKPOINT_STEPS == 0:
                checkpoint_path = os.path.join(save_dir, f'hst_hybrid_step_{step}.pt')
                print(f"\n--- Saving checkpoint to {checkpoint_path} ---")
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'step': step,
                    'current_loss': current_loss_print
                }, checkpoint_path)
                print("--- Saved. ---\n")

except KeyboardInterrupt:
    current_loss_print = current_loss * GRADIENT_ACCUMULATION_STEPS
    print(f"\nInterrupted. Saving at step {step}...")
    interrupt_checkpoint_path = os.path.join(save_dir, 'hst_hybrid_interrupted_latest.pt')
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'step': step,
        'current_loss': current_loss_print
    }, interrupt_checkpoint_path)
    print(f"Saved to: {interrupt_checkpoint_path}")
    sys.exit(0)
except Exception as e:
    current_loss_print = current_loss * GRADIENT_ACCUMULATION_STEPS
    print(f"\nError after {step} steps: {e}. Saving...")
    interrupt_checkpoint_path = os.path.join(save_dir, 'hst_hybrid_interrupted_latest.pt')
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'step': step,
        'current_loss': current_loss_print
    }, interrupt_checkpoint_path)
    print(f"Saved to: {interrupt_checkpoint_path}")
    raise

print("\nTraining complete. Saving final...")
torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(), 'step': step}, os.path.join(save_dir, 'hst_hybrid_final.pt'))

# ==================== GENERATION TEST ====================
print(f"\n[6/7] Speculative gen test ({MAX_GEN_TOKENS} tokens) for TPS...")
PROMPT = "The HST Hybrid model integrates advanced lattice processing and speculative decoding for unprecedented speed on T4 GPUs, enabling rapid generation of coherent long-form text."
try:
    generated_text, stats = generate_speculative(
        model,
        tokenizer,
        PROMPT,
        max_new_tokens=MAX_GEN_TOKENS,
        chunk_size=CHUNK_SIZE
    )
    print("\n======================================================================")
    print(f"HST-Hybrid ULTRA-FAST GENERATION - RESULTS")
    print("======================================================================")
    print(f"Tokens Generated: {stats['tokens_generated']}")
    print(f"Total Time: {stats['total_time']:.2f} seconds")
    print(f"Average TPS: {stats['average_tps']:.2f} (Target: >2000 TPS)")
    print(f"STATUS: {'SUCCESS' if stats['average_tps'] > 2000 else 'OPTIMIZE FURTHER'}")
    print("======================================================================")

    with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
        f.write(generated_text)
    print(f"Text saved to: {OUTPUT_FILENAME}")

except Exception as e:
    print(f"\nGen failed: {e}")

# Cleanup
del model, optimizer, scaler, loader
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
print("Execution complete. Run in Colab T4 for benchmark.")
```